{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11111\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "import csv\n",
    "import cPickle as pickle\n",
    "print(\"11111\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary length: 40000\n",
      "WARNING:tensorflow:From <ipython-input-2-6378b5b36af2>:8: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Tensor(\"embeddings/Shape:0\", shape=(2,), dtype=int32)\n",
      "INFO:tensorflow:Restoring parameters from ../embeddings/final_embeddings/model.ckpt\n",
      "new_embeddings: [[ 0.10012653 -0.0714228  -0.02289733 ...  0.12032924 -0.00925253\n",
      "   0.10401342]\n",
      " [-0.04890159 -0.03326475  0.11969578 ...  0.04146247 -0.08580523\n",
      "  -0.07447449]\n",
      " [-0.05911008  0.00246984 -0.01628511 ...  0.03341895  0.01533006\n",
      "   0.10097369]\n",
      " ...\n",
      " [-0.10917597 -0.02065859  0.03950845 ... -0.04358016 -0.05217595\n",
      "  -0.07356514]\n",
      " [-0.05918077  0.04198439  0.1179584  ... -0.07828391 -0.10345297\n",
      "  -0.06059229]\n",
      " [ 0.08967408 -0.06848793  0.07969988 ... -0.09244967  0.06966498\n",
      "   0.08792025]]\n",
      "Tensor(\"embeddings/Shape_1:0\", shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dictionary = OrderedDict()\n",
    "with open(\"../embeddings/final_embeddings/vocab.txt\", \"r\") as f:\n",
    "    dictionary = pickle.load(f)\n",
    "print(\"dictionary length: %d\" % len(dictionary))\n",
    "#print(\"dictionary: %s\" % dictionary)\n",
    "with tf.variable_scope(\"embeddings\", reuse=tf.AUTO_REUSE):\n",
    "    new_embeddings = tf.get_variable(\"embeddings\", shape = [40000,200])#[vocabulary_size, embedding_size]\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(new_embeddings), 1, keep_dims=True))\n",
    "    restore = tf.train.Saver({\"embeddings\": new_embeddings})\n",
    "    print (tf.shape(new_embeddings))\n",
    "    with tf.Session() as sess:\n",
    "        restore.restore(sess, \"../embeddings/final_embeddings/model.ckpt\")\n",
    "        norm_embeddings = new_embeddings / norm\n",
    "        print(\"new_embeddings: %s\" %norm_embeddings.eval())\n",
    "        final_embeddings = norm_embeddings.eval()\n",
    "        print (tf.shape(norm_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class0_file = \"../data/class-0.csv\"\n",
    "class1_file = \"../data/class-1.csv\"\n",
    "# class2_file = \"../data/class-2.csv\"\n",
    "\n",
    "testing_file = \"../data/test.csv\"\n",
    "\n",
    "input_x = tf.placeholder(tf.int32, [None, None], name=\"input_x\")\n",
    "#y_labels = tf.placeholder(tf.float32, [None, 3], name=\"labels\")\n",
    "y_labels = tf.placeholder(tf.float32, [None, 1], name=\"labels\")\n",
    "dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "testing_x = tf.placeholder(tf.int32, [None, None], name=\"extension_lhs\")\n",
    "\n",
    "\n",
    "def load_data_and_labels():\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    \n",
    "    # Load data from files\n",
    "    x_class0 = list(open(class0_file, \"r\").readlines())\n",
    "    print(\"size x_class0: \"+ str(len(x_class0)))\n",
    "    x_class1 = list(open(class1_file, \"r\").readlines())\n",
    "    print(\"size x_class1: \"+ str(len(x_class1)))\n",
    "#     x_class2 = list(open(class2_file, \"r\").readlines())\n",
    "#     print(\"size x_class2: \"+ str(len(x_class2)))\n",
    "    \n",
    "#     x_testing = list(open(testing_file, \"r\").readlines())\n",
    "#     x_testing = [x.lower() for x in x_testing]\n",
    "   \n",
    "    # Split by words\n",
    "    #x_text = x_class0 + x_class1 + x_class2\n",
    "    x_text = x_class0 + x_class1\n",
    "    x_text = [x.lower() for x in x_text]\n",
    "    print(\"leng(x_test): \" + str(len(x_text)))\n",
    "    \n",
    "    \n",
    "    #x_text = [clean_str(sent) for sent in x_text]\n",
    "    \n",
    "    # Generate labels\n",
    "#     class_2_labels = [[0,0,1] for _ in x_class2]\n",
    "#     class_1_labels = [[0,1,0] for _ in x_class1]\n",
    "#     class_0_labels = [[1,0,0] for _ in x_class0]\n",
    "    #class_4_labels = [[1,0,0,0] for _ in lhs_class4]\n",
    "    \n",
    "    class_1_labels = [[1] for _ in x_class1]\n",
    "    class_0_labels = [[0] for _ in x_class0]\n",
    "    \n",
    "    #y = np.concatenate([class_0_labels, class_1_labels, class_2_labels], 0)\n",
    "    \n",
    "    y = np.concatenate([class_0_labels, class_1_labels],0)\n",
    "#     return [x_text, y, x_testing]\n",
    "    return [x_text, y]\n",
    "\n",
    "\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "size x_class0: 2607\n",
      "size x_class1: 1186\n",
      "leng(x_test): 3793\n",
      "len(x_train):\n",
      "3793\n",
      "WARNING:tensorflow:From <ipython-input-4-9ac9a5792a26>:43: __init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /Users/xueling/anaconda3/envs/python-2-7/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: __init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /Users/xueling/anaconda3/envs/python-2-7/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "[[ 103   18    4 ...    0    0    0]\n",
      " [1233   53   51 ...    0    0    0]\n",
      " [  33  396   23 ...    0    0    0]\n",
      " ...\n",
      " [  51   44  240 ...    0    0    0]\n",
      " [  43  237   97 ...    0    0    0]\n",
      " [  51  240   44 ...    0    0    0]]\n",
      "len(x_train):\n",
      "3414\n",
      "Vocabulary Size: 6756\n",
      "Train/Test split: 3414/379\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data loading params\n",
    "dev_sample_percentage = .1\n",
    "\n",
    "# Model Hyperparameters\n",
    "embedding_dim=200 #128\n",
    "filter_sizes= \"1,2,3\"\n",
    "# num_filters =128\n",
    "num_filters =16\n",
    "dropout_keep_prob=1\n",
    "learning_rate=.001\n",
    "# learning_rate=.0008\n",
    "num_classes=2\n",
    "num_epochs=10\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "batch_size=50\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "# x_train, y, x_test= load_data_and_labels()\n",
    "x_train, y = load_data_and_labels()\n",
    "print(\"len(x_train):\")\n",
    "print(len(x_train))\n",
    "\n",
    "# Build vocabulary  #my data \n",
    "x_text = x_train\n",
    "max_phrase_length = max([len(x.split(\" \")) for x in x_text])\n",
    "vocab_processor_x = learn.preprocessing.VocabularyProcessor(max_phrase_length)\n",
    "x = np.array(list(vocab_processor_x.fit_transform(x_text)))\n",
    "\n",
    "\n",
    "# x_train = x[0:(len(x_train)),:]\n",
    "# x_test = x[len(x_train):len(x),:]\n",
    "\n",
    "\n",
    "\n",
    "## Extract word:id mapping from the object.\n",
    "## Treat the id's as index into list and create a list of words in the ascending order of id's\n",
    "## word with id i goes at index i of the list.\n",
    "vocab_dict = vocab_processor_x.vocabulary_._mapping\n",
    "sorted_vocab= sorted(vocab_dict.items(), key = lambda x : x[1])\n",
    "vocabulary = list(list(zip(*sorted_vocab))[0])\n",
    "\n",
    "\n",
    "# Randomly shuffle data (x_train, y)\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffle = x[shuffle_indices]\n",
    "y_shuffle = y[shuffle_indices]\n",
    "dev_sample_index = -1 * int(dev_sample_percentage * float(len(y)))\n",
    "x_train, x_test = x_shuffle[:dev_sample_index], x_shuffle[dev_sample_index:]\n",
    "y_train, y_test = y_shuffle[:dev_sample_index], y_shuffle[dev_sample_index:]\n",
    "\n",
    "print(x_train)\n",
    "print(\"len(x_train):\")\n",
    "\n",
    "print (len(x_train))\n",
    "\n",
    "# no need\n",
    "# class0_count = 0\n",
    "# class1_count = 0\n",
    "# class2_count = 0\n",
    "# classes_count = np.argmax(y_train,1)\n",
    "# for i in classes_count:\n",
    "#     if i == 0:\n",
    "#         class0_count = class0_count + 1\n",
    "#     elif i == 1:\n",
    "#         class1_count = class1_count + 1\n",
    "#     elif i == 2:\n",
    "#         class2_count = class2_count + 1\n",
    "# total_count = len(y_train)\n",
    "# no need \n",
    "\n",
    "del x, y\n",
    "\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor_x.vocabulary_)))\n",
    "#print(\"RHS Vocabulary Size: {:d}\".format(len(vocab_processor_rhs.vocabulary_)))\n",
    "print(\"Train/Test split: {:d}/{:d}\".format(len(y_train), len(x_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_neural_network(input_x, sequence_length, vocab_size,\n",
    "      embedding_size, filter_sizes, num_filters, initW): \n",
    "   # initW: sub embedding \n",
    "        #W = tf.Variable(\n",
    "               #tf.random_uniform([len(vocab_processor_x.vocabulary_), 128], -1.0, 1.0),\n",
    "               #name=\"W\")\n",
    "        #W.assign(initW)\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            embedded_chars = tf.nn.embedding_lookup(initW, input_x)\n",
    "            embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)\n",
    "            \n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.device('/cpu:0'), tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                #print(embedded_chars_expanded.dtype)\n",
    "                #print(W.dtype)\n",
    "                #print(b.dtype)\n",
    "                conv = tf.nn.conv2d(\n",
    "                    embedded_chars_expanded,\n",
    "                    W,\n",
    "                    use_cudnn_on_gpu=False,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.tanh(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                #print(h.dtype)\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "                \n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        h_pool = tf.concat(pooled_outputs, 3)\n",
    "        h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n",
    "        \n",
    "        #dropout\n",
    "        h_drop = tf.nn.dropout(h_pool_flat, dropout_keep_prob)\n",
    "        \n",
    "        return h_drop, num_filters_total\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6756\n",
      "WARNING:tensorflow:From <ipython-input-6-55b9e45c1cbc>:41: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch 0 completed out of 10 loss: 298.3335052654147 training accuracy: 0.071428575\n",
      "Epoch 1 completed out of 10 loss: 298.283020876348 training accuracy: 0.0\n",
      "Epoch 2 completed out of 10 loss: 302.19557423889637 training accuracy: 0.0\n",
      "Epoch 3 completed out of 10 loss: 329.3487078025937 training accuracy: 0.2857143\n",
      "Epoch 4 completed out of 10 loss: 338.1139939725399 training accuracy: 0.14285715\n",
      "Epoch 5 completed out of 10 loss: 341.5989620089531 training accuracy: 0.0\n",
      "Epoch 6 completed out of 10 loss: 342.9463815689087 training accuracy: 0.14285715\n",
      "Epoch 7 completed out of 10 loss: 343.65974570810795 training accuracy: 0.71428573\n",
      "Epoch 8 completed out of 10 loss: 298.91296216100454 training accuracy: 0.64285713\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "def train_neural_network(input_x):\n",
    "    initW = np.random.uniform(-0.25,0.25,(len(vocab_processor_x.vocabulary_), embedding_dim)).astype(np.float32)\n",
    "    # get the small embedding, subset of the final embedding\n",
    "    for word in vocabulary:\n",
    "        #print(word, dictionary.get(word,0))\n",
    "        #print(final_embeddings[dictionary.get(word,0)])\n",
    "        index = vocab_processor_x.vocabulary_.get(word)\n",
    "        initW[index] = final_embeddings[dictionary.get(word,0)]\n",
    "        #print('index', index)\n",
    "    print(len(vocabulary))\n",
    "   # print(vocabulary)\n",
    "    \n",
    "    # mnist cnn \n",
    "    vector_cnn,num_filters_total = convolutional_neural_network(input_x,sequence_length=x_train.shape[1],\n",
    "            #num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor_x.vocabulary_),\n",
    "            embedding_size=embedding_dim,\n",
    "            filter_sizes=list(map(int, filter_sizes.split(\",\"))),\n",
    "            num_filters=num_filters, initW = initW)\n",
    "    \n",
    "    \n",
    "    dirW = tf.Variable(tf.random_normal([num_filters_total, num_classes]))\n",
    "    bias = tf.Variable(tf.random_normal([num_classes]))\n",
    "    prediction = tf.nn.sigmoid(tf.matmul(vector_cnn,dirW) + bias)\n",
    "    \n",
    "    #weighted loss\n",
    "    #class_weights = tf.constant([[(class0_count)/total_count, class1_count/total_count, class2_count/total_count]])\n",
    "    #print(\"total_count: \" + str(total_count))\n",
    "#     class_weights = tf.constant([[(class0_count)/total_count, class1_count/total_count]])\n",
    "#     print(class_weights)\n",
    "    #weight_per_label = tf.transpose(tf.matmul(y_labels, tf.transpose(class_weights)))\n",
    "    xent = tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y_labels)\n",
    "    \n",
    "    cost = tf.reduce_mean(xent)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    with tf.name_scope('accuracy'):\n",
    "        with tf.name_scope('correct_prediction'):\n",
    "            pred = tf.argmax(prediction, 1)\n",
    "            prob_max = tf.reduce_max(prediction, reduction_indices=[1])\n",
    "            y = tf.argmax(y_labels,1)\n",
    "            correct = tf.equal(pred, y)\n",
    "        with tf.name_scope('accuracy'):\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "    hm_epochs = num_epochs\n",
    "    with tf.Session() as sess:   \n",
    "        tf.global_variables_initializer().run()\n",
    "        for epoch in range(hm_epochs):\n",
    "            batches = batch_iter(list(zip(x_train, y_train)), batch_size, hm_epochs)\n",
    "            #print(epoch, hm_epochs)\n",
    "            epoch_loss = 0\n",
    "            for batch in batches:\n",
    "                #print(batch, batches)\n",
    "                x_batch, y_batch = zip(*batch)\n",
    "                _,c, acc, = sess.run([optimizer,cost,accuracy], feed_dict={input_x:x_batch, y_labels: y_batch})\n",
    "                epoch_loss += c\n",
    "            \n",
    "            \n",
    "            # Record training set loss and accuracy for each epoch during training      \n",
    "            print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss, 'training accuracy:',acc)\n",
    "        print('Testing,accuracy:',accuracy.eval({input_x:x_test,y_labels:y_test}))\n",
    "\n",
    "            #print('Testing Accuracy:',accuracy.eval({x:mnist.test.images,y:mnist.test.labels}))\n",
    "            \n",
    "#             if epoch == hm_epochs-1:\n",
    "                \n",
    "#                 #compute the probablities for the test set\n",
    "#                 csvFile = open('test-set-predictions-cnn-model.csv', 'w')\n",
    "#                 csvFile.write('ID,TITLE,TOPIC\\n')\n",
    "#                 probablities, predictions  = sess.run([prob_max,pred], feed_dict={input_x:x_test})\n",
    "#                 print (len(probablities))\n",
    "#                 print (probablities)\n",
    "#                 index = 0\n",
    "#                 for index in range(len(probablities)):\n",
    "#                     headline = \"\"\n",
    "#                     for i in range(len(x_test[index])):\n",
    "#                         if(x_test[index][i] == 0):\n",
    "#                             break\n",
    "#                         else:\n",
    "#                             headline = headline + vocabulary[x_test[index][i]] + \" \"\n",
    "                    \n",
    "#                     if predictions[index] == 0:\n",
    "#                         p = \"0\"\n",
    "# #                     if predictions[index] == 1:\n",
    "# #                         p = \"1\"\n",
    "#                     elif predictions[index] == 1:\n",
    "#                         p = \"1\"\n",
    "                    \n",
    "#                     row = str(index) + \",\" + headline.strip()  + \",\" + str(predictions[index]) + \"\\n\"\n",
    "#                     csvFile.write(row)\n",
    "#                 csvFile.close()\n",
    "\n",
    "\n",
    "train_neural_network(input_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
